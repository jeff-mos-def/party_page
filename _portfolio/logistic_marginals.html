<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />


<meta name="author" content="Sven Halvorson" />

<meta name="date" content="2020-03-10" />

<title>Marginal Standardization and Visualizing Logistic Regression Through Time</title>

<script src="logistic_marginals_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="logistic_marginals_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="logistic_marginals_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="logistic_marginals_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="logistic_marginals_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="logistic_marginals_files/navigation-1.1/tabsets.js"></script>
<link href="logistic_marginals_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="logistic_marginals_files/highlightjs-9.12.0/highlight.js"></script>
<script src="logistic_marginals_files/kePrint-0.0.1/kePrint.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Marginal Standardization and Visualizing Logistic Regression Through Time</h1>
<h4 class="author">Sven Halvorson</h4>
<h4 class="date">2020-03-10</h4>

</div>

<div id="TOC">
<ul>
<li><a href="#description-of-marginal-standardization">Description of marginal standardization</a></li>
<li><a href="#computing-marginally-adjusted-probabilities-with-larger-models">Computing marginally adjusted probabilities with larger models</a></li>
<li><a href="#plotting-logistic-regression">Plotting logistic regression</a><ul>
<li><a href="#uncorrelated-predictors">Uncorrelated predictors</a></li>
<li><a href="#time-correlated-predictors">Time correlated predictors</a></li>
<li><a href="#time-and-outcome-correlated-predictors">Time and outcome correlated predictors</a></li>
</ul></li>
</ul>
</div>

<p>After working on some problems involving trying to visualize logistic models, my teammate gave me this <a href="https://pdfs.semanticscholar.org/6807/8f0a3839b8d89cd25a964c03a571545793cb.pdf?_ga=2.241765888.362902951.1583340021-1552661845.1583340021">this paper</a>. It did help improve the plots I was making but there are some questions left unanswered. My goal for this document is to:</p>
<ol style="list-style-type: decimal">
<li>Describe marginal standardization &amp; give a simple example.</li>
<li>Demonstrate a calculation for a logistic model with multiple categorical and continuous predictors.</li>
<li>Simulate a data set and look at how the correlation, incidence, and number of predictors affect graphs of a continuous predictor in a logistic model.</li>
<li>Try to draw some conclusions about how to best create graphs for segmented regression models with binary outcomes.</li>
</ol>
<div id="description-of-marginal-standardization" class="section level3">
<h3>Description of marginal standardization</h3>
<p>As a preliminary step, let’s go through the example the authors present of using a logistic model on a population where half the participants are men and half women. Unexposed women have the even 50% of the time and unexposed men have it 99% of the time:</p>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
female
</th>
<th style="text-align:right;">
outcome
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
99
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
50
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
50
</td>
</tr>
</tbody>
</table>
<p>Now we make the model:</p>
<pre class="r"><code>gender_mod = glm(
  outcome ~  female,
  family = &#39;binomial&#39;,
  data = gender_dat
)</code></pre>
<p><strong>Method 1: Marginal standardization</strong></p>
<p>We predict the probability for each gender, then weight them according to their frequency in the data set.</p>
<pre class="r"><code>marg_prob = predict.glm(
  object = gender_mod,
  newdata = tibble(female = c(0,1)),
  type = &#39;response&#39;
)

marg_prob = sum(
  c(0.5, 0.5)*marg_prob
)
round(100*marg_prob, 1)</code></pre>
<pre><code>## [1] 74.5</code></pre>
<p><strong>Method 2: Prediction at the mode of gender</strong></p>
<p>The authors suggest another method which is predicts at the mode of all the categorical predictors. I didn’t even consider this as it seems obvious that it’s going to completely fail unless the population is very homogeneous. It’s also not clear how you would even use that in the case of a 50-50 split like our example. NEXT!</p>
<p><strong>Method 3: Prediction at the mean of gender</strong></p>
<pre class="r"><code>mean_prob = predict.glm(
  object = gender_mod,
  newdata = tibble(female = 0.5),
  type = &#39;response&#39;
) %&gt;% 
 unname()

round(100*mean_prob, 1)</code></pre>
<pre><code>## [1] 90.9</code></pre>
<p>So we end up with a wildly different conclusion. What’s the most intuitive way to do this without all this fancy schmantzy logistic regression? Just use the law of total probability:</p>
<pre class="r"><code>ez_calc = 0.5*0.99 + 0.5*0.5
round(100*ez_calc, 1)</code></pre>
<pre><code>## [1] 74.5</code></pre>
<p>This gives the exact same result as the marginal standardization.</p>
<p>So why is this happening? The author doesn’t give a very satisfying answer to <em>why</em> prediction at the mean doesn’t work. They simply state that we’re making predictions from impossible data points. There are no 50% females (at least in the data set).</p>
<p>The most insightful thing that I’ve been able to come up with as to why our intuition is confused is just looking at the algebra. If we compute the overall odds we get this:</p>
<p><span class="math display">\[\textrm{Overall Odds}\frac{99+50}{1+50} = 2.92\]</span></p>
<p>Computing the odds using method 3 starts with the logistic model: <span class="math display">\[\log(\frac{p}{1-p})=\beta_0+\beta_1(\textrm{female = 1})\]</span> <span class="math display">\[\frac{p}{1-p}=e^{\beta_0+\beta_1(\textrm{female = 1})}\]</span> If we substitute in 0.5 for female, we get this: <span class="math display">\[\frac{p}{1-p}=e^{\beta_0+\beta_1\cdot0.5}=e^{\beta_0}\sqrt{e^{\beta_1}} \]</span> So this would be saying that to compute the overall odds, we take the odds of the outcome for men and multiply it by the square root of the odds ratio of women to men. Note the square root is only because we have a 50-50 split of women to men.</p>
<p><span class="math display">\[\frac{p}{1-p}=\frac{99}{1}\sqrt{\frac{\frac{50}{50}}{\frac{99}{1}}} = 9.95\]</span> This gives us the same predicted probability as the code above:</p>
<p><span class="math display">\[\frac{9.95}{1+9.95}=0.909\]</span></p>
<p>It just seems hard to explain why you would want to exponentiate the odds ratio for the second group by the relative frequency of that group in order to compute the overall odds. This isn’t that satisfying either but it does at least make prediction at the mean (method 3) seem more bizarre.</p>
<p>Framing it this way does let us see how, as Muller notes, having a low correlation between the predictor and outcome will cause this to be very similar to the marginal method. If <span class="math inline">\(e^{\beta_1}\approx1\)</span>:</p>
<p><span class="math display">\[\frac{p}{1-p}=e^{\beta_0+\beta_1\cdot P(X=1)} \approx e^{\beta_0}\cdot 1^{P(X =1)}\approx e^{\beta_0}\]</span> And if the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is very weak, the odds of <span class="math inline">\(Y\)</span> should be about the same when <span class="math inline">\(X = 0\)</span> as when <span class="math inline">\(X = 1\)</span>.</p>
</div>
<div id="computing-marginally-adjusted-probabilities-with-larger-models" class="section level3">
<h3>Computing marginally adjusted probabilities with larger models</h3>
<p>Computing the marginally adjusted probability is pretty easy with one predictor but programming the calculation for a more complex model might not seem so simple. Here I’ll show an example with a variation on the <code>nycflights13:flights</code> data set that you can get through the corresponding library. It has a list of a lot of flights out of NYC in 2013. We’ll make our outcome variable whether or not the flight departed late and use the departing airport, flight distance, and whether the scheduled departure was in the afternoon. I’ve dropped observations with a missing outcome or predictor. I’ve also made dummies for the airport where Newark is the reference category.</p>
<p>Here’s some of the data set we’ll use:</p>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
late
</th>
<th style="text-align:right;">
origin_LGA
</th>
<th style="text-align:right;">
origin_JFK
</th>
<th style="text-align:right;">
sched_dep_time
</th>
<th style="text-align:right;">
pm
</th>
<th style="text-align:right;">
distance
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
640
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
319
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1030
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1416
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1316
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
116
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
715
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
888
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1940
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1010
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1700
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
273
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1638
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
583
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1815
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
2133
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
800
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
2586
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1021
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1028
</td>
</tr>
</tbody>
</table>
<p>Here’s the model we’ll use:</p>
<pre class="r"><code>flight_mod = glm(
  late ~  origin_LGA + origin_JFK + pm + distance,
  family = &#39;binomial&#39;,
  data = flights_ex
)</code></pre>
<p>For reference, we can compute the probability of leaving on time:</p>
<pre class="r"><code>round(100*mean(flights_ex[[&#39;late&#39;]]), 1)</code></pre>
<pre><code>## [1] 39.1</code></pre>
<p>To use the marginal standardization method, we will use the law of total probability again. This time there are 6 possible combinations (3 airports, am or pm) of the categorical variables that we have to sum across. Start by computing the relative frequencies of each combination within our data set:</p>
<pre class="r"><code>rel_freq = flights_ex %&gt;% 
  select(
    origin_LGA,
    origin_JFK,
    pm
  ) %&gt;% 
  group_by_all() %&gt;% 
  count() %&gt;% 
  ungroup() %&gt;% 
  mutate(
    rel_freq = n/sum(n)
  )</code></pre>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
origin_LGA
</th>
<th style="text-align:right;">
origin_JFK
</th>
<th style="text-align:right;">
pm
</th>
<th style="text-align:left;">
n
</th>
<th style="text-align:right;">
rel_freq
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
46,972
</td>
<td style="text-align:right;">
0.143
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
70,624
</td>
<td style="text-align:right;">
0.215
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
43,410
</td>
<td style="text-align:right;">
0.132
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
58,099
</td>
<td style="text-align:right;">
0.177
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
38,560
</td>
<td style="text-align:right;">
0.117
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
70,856
</td>
<td style="text-align:right;">
0.216
</td>
</tr>
</tbody>
</table>
<p>Now we take this matrix, add on the means of the continuous variables, and then make the predictions from the model:</p>
<pre class="r"><code>rel_freq = rel_freq %&gt;% 
  mutate(
    distance = mean(flights_ex[[&#39;distance&#39;]])
  )
rel_freq[&#39;predicted_prob&#39;] = predict.glm(
  flight_mod,
  rel_freq,
  type = &#39;response&#39;
)</code></pre>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
origin_LGA
</th>
<th style="text-align:right;">
origin_JFK
</th>
<th style="text-align:right;">
pm
</th>
<th style="text-align:left;">
n
</th>
<th style="text-align:right;">
rel_freq
</th>
<th style="text-align:right;">
distance
</th>
<th style="text-align:right;">
predicted_prob
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
46,972
</td>
<td style="text-align:right;">
0.143
</td>
<td style="text-align:right;">
1048.571
</td>
<td style="text-align:right;">
0.306
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
70,624
</td>
<td style="text-align:right;">
0.215
</td>
<td style="text-align:right;">
1048.571
</td>
<td style="text-align:right;">
0.542
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
43,410
</td>
<td style="text-align:right;">
0.132
</td>
<td style="text-align:right;">
1048.571
</td>
<td style="text-align:right;">
0.217
</td>
</tr>
<tr>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
58,099
</td>
<td style="text-align:right;">
0.177
</td>
<td style="text-align:right;">
1048.571
</td>
<td style="text-align:right;">
0.427
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
38,560
</td>
<td style="text-align:right;">
0.117
</td>
<td style="text-align:right;">
1048.571
</td>
<td style="text-align:right;">
0.238
</td>
</tr>
<tr>
<td style="text-align:right;">
0
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
70,856
</td>
<td style="text-align:right;">
0.216
</td>
<td style="text-align:right;">
1048.571
</td>
<td style="text-align:right;">
0.456
</td>
</tr>
</tbody>
</table>
<p>We can see that the you have a more than 50% chance to leave late on flights from Newark in the afternoon and only a 21% chance on a morning flight from LaGuardia. This difference will make the marginal method better than the means method.</p>
<p>Final step is to just compute the weighted average of these predicted probabilities:</p>
<pre class="r"><code>rel_freq %&gt;% 
  summarize(
    marg_predicted_prob = sum(rel_freq*predicted_prob)
  )</code></pre>
<pre><code>## # A tibble: 1 x 1
##   marg_predicted_prob
##                 &lt;dbl&gt;
## 1               0.391</code></pre>
<p>It’s exactly the same as the computation without the model. What does the prediction at the means method yield? First compute the means of each variable:</p>
<pre class="r"><code>flight_means = flights_ex %&gt;% 
  select(
    origin_LGA,
    origin_JFK,
    pm,
    distance
  ) %&gt;% 
  summarize_all(
    mean
  )</code></pre>
<table class="table table-striped table-hover" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
origin_LGA
</th>
<th style="text-align:right;">
origin_JFK
</th>
<th style="text-align:right;">
pm
</th>
<th style="text-align:right;">
distance
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
0.3089879
</td>
<td style="text-align:right;">
0.3330563
</td>
<td style="text-align:right;">
0.6075076
</td>
<td style="text-align:right;">
1048.571
</td>
</tr>
</tbody>
</table>
<p>Then use this as your only data point to predict:</p>
<pre class="r"><code>predict.glm(
  flight_mod,
  flight_means,
  type = &#39;response&#39;
) %&gt;% 
  unname()</code></pre>
<pre><code>## [1] 0.3830153</code></pre>
<p>This prediction is off by about 0.8 percentage points. My suspicion is that the more categorical predictors we use and the more heavily they are correlated with the outcome, the large the gap between these two methods will become.</p>
<p>You might be wondering why you would even do this if I was able to just compute the probabilities without the models. We’ll look at some graphs in the next section but one reason is to compute other confounder adjusted summaries like a relative risk.</p>
<p>Let’s a compute the adjusted relative risk of leaving late between flights in the am vs pm. This time we’ll exclude the pm flag from our computations of the relative frequencies of each covariate pattern:</p>
<pre class="r"><code>rel_freq_pm = flights_ex %&gt;% 
  select(
    origin_LGA,
    origin_JFK
  ) %&gt;% 
  group_by_all() %&gt;% 
  count() %&gt;% 
  ungroup() %&gt;% 
  mutate(
    rel_freq = n/sum(n)
  )  %&gt;% 
  mutate(
    distance = mean(flights_ex[[&#39;distance&#39;]])
  ) %&gt;% 
  crossing(
    tibble(
      pm = 0:1
    )
  )

  
rel_freq_pm[&#39;predicted_prob&#39;] = predict.glm(
  flight_mod,
  rel_freq_pm,
  type = &#39;response&#39;
)

rel_freq_pm %&gt;% 
  group_by(pm) %&gt;% 
  summarize(
    predicted_prob = sum(rel_freq*predicted_prob)
  ) %&gt;% 
  pivot_wider(
    names_from = pm,
    values_from = predicted_prob,
    names_prefix = &#39;pm_&#39;
  ) %&gt;% 
  mutate(
    RR = pm_1/pm_0
  )</code></pre>
<pre><code>## # A tibble: 1 x 3
##    pm_0  pm_1    RR
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 0.256 0.478  1.87</code></pre>
<p>This is telling us that our confounder adjusted model predicts that you are 1.87 times as likely to leave late in the afternoon than in the morning.</p>
</div>
<div id="plotting-logistic-regression" class="section level3">
<h3>Plotting logistic regression</h3>
<p>The origin of looking at this problem came from trying to plot logistic models through time overlayed on the yearly/monthly/weekly/ incidence. Sometimes these plots look good and other times they seem quite off. I have questions about when exactly these plots seem to fail. Is any sort of cumulative effect of adding more total confounders? What kinds of correlations cause the graphics to look so ugly?</p>
<p>I’m a considerably better programmer than statistician so I think I’ll simulate a data set and do some experiment to try and address these questions. We’ll get a chance to try out this <code>simstudy</code> library that I saw on a blog. Here’s the <a href="https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html">vignette</a> for reference.</p>
<p>For this experiment I’m going to create three outcome variables with linear log odds changes through time: a low incidence with a small change through time, a medium incidence with a medium change through time, and a high incidence with a large change through time. I’ve created roughly 1,000 observations for each of the 100 time points.</p>
<p><img src="logistic_marginals_files/figure-html/simdat-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>After creating the outcome and time variables, I created 400 binary variables to act as covariates for our models. Each flavor was created in sets of 20. The types were:</p>
<ol style="list-style-type: decimal">
<li>Not correlated with either the time or outcome. These were set at a low, medium, or high incidence like the outcomes for a total of 60 predictors.</li>
<li>Correlated with only the time variable. These were also set at a low, medium, or high incidence. The effect of time on the log odds was increased by 5% each iteration and the direction was reversed every other iteration. This totaled another 60 predictors.</li>
<li>Correlated with both time and the predictor. Here we again set the incidence at low, medium, or high and correlated with exactly one of the outcomes. The strength of the correlations to the outcome and time were increased with each iteration as well. This is an additional 180 predictors.</li>
<li>The medium incidence outcome, medium incidence predictor distributions from part #3 were sampled again 5 times each for the last 100 predictors. This was done to test the effect of the number of predictors, not just correlation with outcome and time.</li>
</ol>
<p>A graph of the predictors that were of medium incidence and correlate with time only are depicted below:</p>
<p><img src="logistic_marginals_files/figure-html/show_timecor-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="uncorrelated-predictors" class="section level4">
<h4>Uncorrelated predictors</h4>
<p>The first thing I want to do is look at some graphs of what happens when you use uncorrelated predictors. We should probably think that this will not affect the shape of the graph very much because the effect on the outcome is constant through time. Here we’ll just look at a graph of the low incidence outcome vs time while we successively. These models were made from randomly sampling the uncorrelated predictors of all incidences:</p>
<p><img src="logistic_marginals_files/figure-html/uncorr_prd-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>All of the lines are basically on top of each other indicating that these graphs aren’t really affected by the number of uncorrelated predictors in the model.</p>
</div>
<div id="time-correlated-predictors" class="section level4">
<h4>Time correlated predictors</h4>
<p>Let’s do something similar here where we progressively add in more predictors that are correlated with the time variable only. I’m going to use the weakest correlations first and progressively move to stronger predictors. We’ll use the medium incidence outcome and randomly select the incidence of the predictors.</p>
<p><img src="logistic_marginals_files/figure-html/time_corr_prd-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This plot shows no difference between the lines either. What happens when you progressively add time correlated predictors?</p>
<p><img src="logistic_marginals_files/figure-html/time_corr_prd2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This is the first example where we can see some separation between the graphs but it’s sill quite minor.</p>
</div>
<div id="time-and-outcome-correlated-predictors" class="section level4">
<h4>Time and outcome correlated predictors</h4>
<p>Here I’ve plotted models with an binary predictor with successively increasing strength. The unadjusted (time only) model is plotted in orange. We can see that the undajusted model is closer to the models with weaker relationships to the outcome and time variables.</p>
<p><img src="logistic_marginals_files/figure-html/time_out_cor_prd-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Now a version with increasing the number and strength of predictors:</p>
<p><img src="logistic_marginals_files/figure-html/time_out_corr_prd2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here we can see that this is getting royally messed up.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
